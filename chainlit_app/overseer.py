import json
from typing import Any, Dict, List, Optional
from utils.llm_client import get_llm_client, get_model_setting

OVERSEER_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹å…ƒèªçŸ¥ç›£ç£è€…ï¼ˆOverseer LLMï¼‰ã€‚
ä½ çš„å·¥ä½œï¼šåœ¨æ¯å€‹å·¥å…·éˆï¼ˆtool chainï¼‰çµæŸå¾Œï¼Œå¯©è¦–ç›®å‰çš„ä»»å‹™é€²åº¦ã€å·²å˜—è©¦çš„æ–¹æ³•èˆ‡çµæœã€æ˜¯å¦æœ‰éºæ¼å­ä»»å‹™ï¼Œä¸¦æå‡ºä¸‹ä¸€æ­¥ç­–ç•¥æˆ–çµ‚æ­¢å»ºè­°ã€‚
è«‹å‹™å¿…ä»¥ JSON è¼¸å‡ºï¼Œéµå®ˆæˆ‘æä¾›çš„ schemaã€‚"""

OVERSEER_USER_PROMPT = """ä»»å‹™ç›®æ¨™ï¼ˆä½¿ç”¨è€…æœ€åˆéœ€æ±‚ï¼‰ï¼š
{goal}

ç›®å‰çš„è¡Œå‹•ç´€éŒ„ï¼ˆå« LLM æ€è€ƒã€å·¥å…·å‘¼å«èˆ‡çµæœï¼ŒæŒ‰æ™‚é–“åºï¼Œå·²æˆªæ–·è‡³å®‰å…¨é•·åº¦ï¼‰ï¼š
{trace}

è«‹å‹™å¿…è¼¸å‡º JSONï¼Œéµå®ˆä»¥ä¸‹éµï¼š
status: "continue" | "need_user_input" | "terminate"
progress_summary: å°ç›®å‰é€²åº¦çš„ç²¾ç°¡ç¸½çµ
missing_or_interrupted_subtasks: å°šæœªå®Œæˆ/è¢«éºå¿˜çš„å­ä»»å‹™åˆ—è¡¨
tried_paths: åˆ—å‡ºå·²å˜—è©¦æ–¹æ³•ã€çµæœï¼ˆsuccess/fail/partialï¼‰ã€ä»¥åŠåŸå› 
next_actions: è‹¥ status=continueï¼Œè«‹åˆ—å‡ºå»ºè­°çš„ä¸‹ä¸€æ­¥è¡Œå‹•ï¼ˆå¯å«æ‡‰å‘¼å«çš„ tool èˆ‡å…¶ argsï¼‰
loop_or_blocker_detected: æ˜¯å¦åµæ¸¬åˆ°å¡ä½/é‡è¤‡å˜—è©¦çš„å¾ªç’°ï¼Œä»¥åŠå»ºè­°
ask_user: éœ€è¦ç”¨æˆ¶æä¾›å“ªäº›è³‡è¨Šï¼ˆè‹¥ status=need_user_inputï¼‰
final_recommendation_to_user: è‹¥ status=terminate æ™‚ï¼Œè«‹å°ä½¿ç”¨è€…æå‡ºæ˜ç¢ºçš„ä¸‹ä¸€æ­¥å»ºè­°
ç¯„ä¾‹:
{{
  "status": "continue | need_user_input | terminate",
  "progress_summary": "string",
  "missing_or_interrupted_subtasks": ["string", "..."],
  "tried_paths": [
    {{"approach": "string", "result": "success | fail | partial", "why": "string"}}
  ],
  "next_actions": [
    {{
      "rationale": "string",
      "proposed_tool": "string|null",
      "proposed_args": {{}},
      "expected_outcome": "string"
    }}
  ],
  "loop_or_blocker_detected": {{
    "detected": true,
    "reason": "string",
    "suggestion": "string"
  }},
  "ask_user": "null | string (éœ€è¦ä½¿ç”¨è€…æä¾›çš„ç²¾ç¢ºè³‡æ–™æˆ–æ±ºç­–)",
  "final_recommendation_to_user": "null | string (è‹¥ status=terminate æ™‚çµ¦ä½¿ç”¨è€…çš„ä¸‹ä¸€æ­¥æŒ‡ç¤º)"
}}
è«‹åªè¼¸å‡ºå–®ä¸€ JSONï¼Œä¸”ä¸å¾—åŒ…å«å¤šé¤˜æ–‡å­—ã€‚"""

def get_overseer_model_setting():
    params = get_model_setting()
    # ä½¿ç”¨è¼ƒä¾¿å®œæˆ–æ“…é•·æ¨ç†/é•·ä¸Šä¸‹æ–‡çš„æ¨¡å‹
    params.update({
        "model": "gpt-4o-mini",   # èˆ‰ä¾‹ï¼Œè«‹æ›æˆä½ å¯¦éš›è¦ç”¨çš„
        "temperature": 0.2,
    })
    # Overseer æˆ‘å€‘ä¸€èˆ¬ä¸éœ€è¦ streaming
    params.pop("stream", None)
    return params

def build_tool_trace(message_history: List[Dict[str, Any]], max_chars: int = 12000) -> str:
    """
    å¾ message_history ä¸­æ“·å–åŒ…å«ï¼š
    - ä½¿ç”¨è€…åŸå§‹ç›®æ¨™ï¼ˆå¯å¾ system / user é–‹é ­ç¬¬ä¸€å‰‡å–å‡ºï¼‰
    - æ¯å€‹ assistant -> tool_calls
    - æ¯å€‹ tool çš„çµæœ
    ä¸¦å£“ç¸®åˆ° max_chars å…§ã€‚
    """
    # ä½ ä¹Ÿå¯ä»¥æ˜ç¢ºåªæ“·å– role in ['assistant','tool'] çš„ç‰‡æ®µ
    buf = []
    for msg in message_history:
        role = msg.get("role")
        if role == "user":
            buf.append(f"[user] {msg.get('content','')}")
        elif role == "assistant":
            # å¦‚æœé€™æ˜¯ä¸€èˆ¬å›ç­”
            if msg.get("content"):
                buf.append(f"[assistant] {msg['content']}")
            # å¦‚æœåŒ…å« tool_callsï¼Œå¯è¨˜éŒ„åç¨± & å¼•æ•¸ï¼ˆé¿å…å¤ªé•·ï¼Œå¿…è¦å¯æˆªæ–·ï¼‰
            if "tool_calls" in msg and msg["tool_calls"]:
                for tc in msg["tool_calls"]:
                    name = tc["function"]["name"]
                    args = tc["function"]["arguments"]
                    buf.append(f"[assistant->tool_call] {name}({args})")
        elif role == "tool":
            tcid = msg.get("tool_call_id", "")
            content = msg.get("content", "")
            if len(content) > 1000:
                content = content[:1000] + "...[truncated]"
            buf.append(f"[tool#{tcid}] {content}")
    trace = "\n".join(buf)
    if len(trace) > max_chars:
        trace = trace[-max_chars:]  # ä¿ç•™æœ€å¾Œä¸€æ®µæœ€è²¼è¿‘ç¾æ³
    return trace

async def run_overseer(goal: str, message_history: List[Dict[str, Any]]) -> Dict[str, Any]:
    llm_client = get_llm_client(mode="async")  # ä½ åŸæœ¬çš„å–ç”¨æ–¹å¼
    params = get_overseer_model_setting()

    trace = build_tool_trace(message_history)

    messages = [
        {"role": "system", "content": OVERSEER_SYSTEM_PROMPT},
        {"role": "user", "content": OVERSEER_USER_PROMPT.format(goal=goal, trace=trace)},
    ]

    resp = await llm_client.chat.completions.create(
        messages=messages,
        **params,
        response_format={"type": "json_object"},
    )
    content = resp.choices[0].message.content 
    try:
        return json.loads(content)
    except Exception:
        # å¾Œå‚™ï¼šè‹¥ LLM æ²’æœ‰å›å‚³åˆæ³• JSONï¼Œå˜—è©¦ç°¡å–®ä¿®å¾©æˆ–å›å‚³ fallback
        return {
            "status": "terminate",
            "progress_summary": "Overseer å›å‚³é JSONï¼Œç„¡æ³•è§£æã€‚",
            "missing_or_interrupted_subtasks": [],
            "tried_paths": [],
            "next_actions": [],
            "loop_or_blocker_detected": {"detected": True, "reason": "invalid_json", "suggestion": "è«‹é‡æ–°å˜—è©¦"},
            "ask_user": None,
            "final_recommendation_to_user": "è«‹æ±‚ä½¿ç”¨è€…æˆ–ç³»çµ±é‡è©¦ Overseer ç¨‹åºã€‚"
        }
    

def render_overseer_for_user(r: Dict[str, Any]) -> str:
    lines = []
    lines.append(f"ğŸ” Overseer ç‹€æ…‹ï¼š{r.get('status')}")
    if r.get("progress_summary"):
        lines.append(f"ğŸ“Œ é€²åº¦ç¸½çµï¼š{r['progress_summary']}")
    if r.get("missing_or_interrupted_subtasks"):
        lines.append("ğŸ§­ å°šæœªå®Œæˆ/éºæ¼çš„å­ä»»å‹™ï¼š")
        for s in r["missing_or_interrupted_subtasks"]:
            lines.append(f"  - {s}")
    if r.get("tried_paths"):
        lines.append("ğŸ§ª å·²å˜—è©¦æ–¹æ³•ï¼š")
        for p in r["tried_paths"]:
            lines.append(f"  - {p['approach']} â†’ {p['result']}ï¼ˆåŸå› ï¼š{p.get('why','')}ï¼‰")
    if r.get("next_actions"):
        lines.append("â¡ï¸ å»ºè­°çš„ä¸‹ä¸€æ­¥ï¼š")
        for a in r["next_actions"]:
            lines.append(f"  - {a['rationale']} | tool: {a.get('proposed_tool')} args: {a.get('proposed_args')}")
    if (lp := r.get("loop_or_blocker_detected")) and lp.get("detected"):
        lines.append(f"âš ï¸ é˜»å¡/æ­»å¾ªç’°åµæ¸¬ï¼š{lp.get('reason')} â†’ å»ºè­°ï¼š{lp.get('suggestion')}")
    if r.get("ask_user"):
        lines.append(f"ğŸ™‹ éœ€è¦ä½¿ç”¨è€…è£œå……ï¼š{r['ask_user']}")
    if r.get("final_recommendation_to_user"):
        lines.append(f"ğŸ§­ çµ¦ä½¿ç”¨è€…çš„æœ€çµ‚å»ºè­°ï¼š{r['final_recommendation_to_user']}")
    return "\n".join(lines)