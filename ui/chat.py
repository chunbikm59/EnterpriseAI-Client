from openai import AsyncOpenAI, OpenAI
import chainlit as cl
from chainlit.input_widget import Select, Switch
from typing import Dict, Any, List, Optional
from mcp.types import CallToolResult, TextContent
import os
import json
import base64
import io
from markitdown import MarkItDown
import datetime
from dotenv import load_dotenv
from utils.mcp_manager import MCPConnectionManager
load_dotenv()

# å¯æ›¿æ›æˆæœ¬åœ°æ¨¡å‹ï¼Œæ¯”å¦‚ä½¿ç”¨ LM Studio çš„ API
BASE_URL = None # "http://localhost:1234/v1"
API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_SETTING = {
    "model": "gpt-4o-mini",
    "temperature": 1,
    "stream": True,
}
# MCP ä¼ºæœå™¨é…ç½® å¯¦éš›å¾è³‡æ–™åº«ä¸­å–å¾—
MCP_SERVERS_CONFIG = {
    "weather_http": {
        "type": "http",
        "url": "http://localhost:8000/mcp-weather/mcp/",
        # "url": "http://localhost:8123/mcp/",
        "enabled": True,
        "description": "å¤©æ°£æŸ¥è©¢ HTTP MCP ä¼ºæœå™¨ç¯„ä¾‹" 
    },
    "sequentialthinking": {
        "type": "stdio", 
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"],
        "enabled": False,
        "description": "å°‡è¤‡é›œå•é¡Œåˆ†è§£ç‚ºå¯ç®¡ç†çš„æ­¥é©Ÿï¼Œéš¨è‘—ç†è§£çš„åŠ æ·±ï¼Œä¿®æ”¹ä¸¦å®Œå–„æƒ³æ³•ã€‚"
    },
    "playwright": {
        "type": "stdio", 
        "command": "npx",
        "args": ["-y", "@playwright/mcp@latest", "--isolated"],
        "enabled": True,
        "description": "ä¸€å€‹ä½¿ç”¨Playwrightæä¾›ç€è¦½å™¨è‡ªå‹•åŒ–åŠŸèƒ½çš„æ¨¡å‹ä¸Šä¸‹æ–‡å”å®š (MCP) ä¼ºæœå™¨ã€‚è©²ä¼ºæœå™¨ä½¿ LLM èƒ½å¤ é€éçµæ§‹åŒ–çš„å¯è¨ªå•æ€§å¿«ç…§èˆ‡ç¶²é é€²è¡Œäº¤äº’ï¼Œè€Œç„¡éœ€ä½¿ç”¨è¢å¹•æˆªåœ–æˆ–è¦–è¦ºèª¿æ•´çš„æ¨¡å‹ã€‚"
    },
    # "filesystem": {
    #     "type": "stdio", 
    #     "command": "npx",
    #     "args": ["-y", "@modelcontextprotocol/server-filesystem"],
    #     "enabled": True,
    #     "description": "Node.js server implementing Model Context Protocol (MCP) for filesystem operations."
    # },
    
}

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

async def check_and_process_new_images(existing_files):
    """æª¢æŸ¥ä¸¦è™•ç†å·¥å…·ç”Ÿæˆçš„æ–°åœ–ç‰‡æª”æ¡ˆ"""
    file_folder = cl.user_session.get('file_folder')
    if not file_folder or not os.path.exists(file_folder):
        return
    
    # æ”¯æ´çš„åœ–ç‰‡æ ¼å¼
    image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.svg'}
    
    # å–å¾—ç•¶å‰æª”æ¡ˆåˆ—è¡¨
    current_files = set(os.listdir(file_folder))
    new_files = current_files - existing_files
    
    # ç¯©é¸å‡ºæ–°çš„åœ–ç‰‡æª”æ¡ˆ
    new_images = [f for f in new_files if os.path.splitext(f.lower())[1] in image_extensions]
    
    if not new_images:
        return
    
    # æº–å‚™åœ–ç‰‡å…ƒç´ å’Œå…§å®¹
    image_elements = []
    image_content = []
    
    for image_file in new_images:
        image_path = os.path.join(file_folder, image_file)
        
        try:
            # å‰µå»ºåœ–ç‰‡å…ƒç´ 
            image_element = cl.Image(
                name=image_file,
                path=image_path,
                display="inline"
            )
            image_elements.append(image_element)
            
            # å°‡åœ–ç‰‡åŠ å…¥åˆ°å…§å®¹ä¸­
            image_content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encode_image(image_path)}",
                    "detail": "high"
                }
            })
            
        except Exception as e:
            print(f"è™•ç†åœ–ç‰‡ {image_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    # ä¸€æ¬¡ç™¼é€æ‰€æœ‰åœ–ç‰‡åˆ° UI
    if image_elements:
        await cl.Message(
            content='',
            elements=image_elements
        ).send()
        
        # å°‡æ‰€æœ‰åœ–ç‰‡åŠ å…¥åˆ° message_history çš„åŒä¸€å€‹è¨Šæ¯ä¸­
        message_history = cl.user_session.get("message_history", [])
        image_message = {
            "role": "user",
            "content": image_content
        }
        message_history.append(image_message)
        
        # æ›´æ–° session ä¸­çš„ message_history
        cl.user_session.set("message_history", message_history)
    
@cl.step(name="æª”æ¡ˆè§£æ")
async def convert_to_markdown(file_path, model="gpt-4o-mini", use_vision_model=False):
    # æ ¹æ“šè¨­å®šæ±ºå®šæ˜¯å¦ä½¿ç”¨è¦–è¦ºèªè¨€æ¨¡å‹
    if use_vision_model:
        client = OpenAI(base_url=BASE_URL, api_key=API_KEY)  # ä½¿ç”¨åŒæ­¥å®¢æˆ¶ç«¯
        md = MarkItDown(enable_plugins=True, llm_client=client, llm_model=model)
    else:
        md = MarkItDown(enable_plugins=True)
    
    result = md.convert(file_path)    
    
    return result.text_content


@cl.on_chat_start
async def start():
    file_folder = os.path.join(os.getcwd(), '.files', cl.user_session.get('id'))
    os.mkdir(file_folder)
    cl.user_session.set('file_folder', file_folder)
    cl.user_session.set(
        "message_history",
        [
            {
                "role": "system",
                "content": "You are a helpful å°ç£ç¹é«”ä¸­æ–‡ AI assistant. You can access tools using MCP servers.",
            },
            {
                "role": "assistant",
                "content": f"ç¾åœ¨æ™‚é–“æ˜¯ {datetime.datetime.now()}",
            }
        ],
    )
    # å»ºç«‹è¨­å®šé¸é …
    settings_widgets = []
    
    # æ–°å¢è¦–è¦ºèªè¨€æ¨¡å‹è¨­å®šé¸é …
    settings_widgets.append(
        Switch(
            id="use_vision_model",
            label="ä½¿ç”¨è¦–è¦ºèªè¨€æ¨¡å‹æè¿°æª”æ¡ˆä¸­çš„åœ–ç‰‡",
            initial=False,
            description='å•Ÿç”¨å¾Œï¼Œåœ¨è§£ææª”æ¡ˆå¦‚PDFã€PPTæ™‚ MarkItDown å°‡ä½¿ç”¨ GPT-4o-mini ç­‰è¦–è¦ºèªè¨€æ¨¡å‹ä¾†åˆ†æå’Œæè¿°æª”æ¡ˆä¸­çš„åœ–ç‰‡å…§å®¹ï¼Œæä¾›æ›´è©³ç´°çš„åœ–ç‰‡èªªæ˜ã€‚ä½†æœƒå¢åŠ é¡å¤–çš„è™•ç†æ™‚é–“èˆ‡ Token è²»ç”¨'
        )
    )

    if "playwright" in MCP_SERVERS_CONFIG:
        MCP_SERVERS_CONFIG.get('playwright')['args'] += [f"--output-dir={file_folder}"] # f"--storage-state={file_folder}", 
    if 'filesystem' in MCP_SERVERS_CONFIG:
        MCP_SERVERS_CONFIG.get('filesystem')['args'].append(file_folder)

    # æ–°å¢ MCP ä¼ºæœå™¨è¨­å®šé¸é …
    for server_name, config in MCP_SERVERS_CONFIG.items():
        settings_widgets.append(
            Switch(
                id=f"mcp_{server_name}",
                label=f"MCP - {server_name}",
                initial=config['enabled']
            )
        )
    
    settings = await cl.ChatSettings(settings_widgets).send()
    cl.user_session.set('chat_setting', settings_widgets)

    mcp_manager = MCPConnectionManager(config=MCP_SERVERS_CONFIG, on_connect=on_mcp_connect)
    cl.user_session.set('mcp_manager', mcp_manager)
    
    # æ ¹æ“šåˆå§‹è¨­å®šé€£ç·šå·²å•Ÿç”¨çš„ä¼ºæœå™¨
    for server_name, config in MCP_SERVERS_CONFIG.items():
        setting_key = f"mcp_{server_name}"
        if settings.get(setting_key, config.get('enabled', False)):
            await mcp_manager.add_connection(server_name, config)

async def on_mcp_connect(name, tools=[]):
    await cl.Message(content=f'âœ…ï¸ å·²é€£ç·š MCP Server: {name} ').send()
    
    # åœ¨è¨­å®šä»‹é¢ä¸­æ›´æ–°è©²MCPçš„é¸é …æè¿°
    chat_setting = cl.user_session.get('chat_setting', [])
    for element in chat_setting:
        if element.id == f"mcp_{name}":
            element.description = ', '.join([f"{t['name']}" for t in tools])
            break
        
    cl.user_session.set('chat_setting', chat_setting)
    settings = await cl.ChatSettings(chat_setting).send()
    
@cl.on_chat_end
async def end():
    mcp_manager = cl.user_session.get('mcp_manager')
    if mcp_manager:
        await mcp_manager.shutdown()
        
@cl.on_settings_update
async def setup_agent(settings):
    """è™•ç†è¨­å®šè®Šæ›´ï¼Œé€£ç·šæˆ–æ–·ç·š MCP ä¼ºæœå™¨"""
    print("è¨­å®šå·²æ›´æ–°:", settings)
    
    # å„²å­˜ç•¶å‰è¨­å®šåˆ° session ä¸­
    cl.user_session.set('current_settings', settings)
    
    mcp_manager = cl.user_session.get('mcp_manager')
    if not mcp_manager:
        return
    
    chat_setting = cl.user_session.get('chat_setting')
    for element in chat_setting:
        element.initial = settings.get(element.id, False)

    # è™•ç†è¦–è¦ºèªè¨€æ¨¡å‹è¨­å®šè®Šæ›´
    use_vision_model = settings.get("use_vision_model", False)
    if use_vision_model:
        await cl.Message(content="âœ… å·²å•Ÿç”¨è¦–è¦ºèªè¨€æ¨¡å‹ä¾†æè¿°æª”æ¡ˆä¸­çš„åœ–ç‰‡").send()
    else:
        await cl.Message(content="âŒ å·²åœç”¨æª”æ¡ˆè§£æçš„åœ–ç‰‡æè¿°åŠŸèƒ½").send()

    # è™•ç†æ¯å€‹ MCP ä¼ºæœå™¨çš„è¨­å®šè®Šæ›´
    for server_name, config in MCP_SERVERS_CONFIG.items():
        setting_key = f"mcp_{server_name}"
        is_enabled = settings.get(setting_key, False)
        is_connected = mcp_manager.is_connected(server_name)
        
        if is_enabled and not is_connected:
            # éœ€è¦é€£ç·šä½†å°šæœªé€£ç·š
            await mcp_manager.add_connection(server_name, config)
            await cl.Message(content=f"ğŸ”— æ­£åœ¨é€£ç·šåˆ° MCP ä¼ºæœå™¨: {server_name}").send()
            
        elif not is_enabled and is_connected:
            # éœ€è¦æ–·ç·šä½†ä»åœ¨é€£ç·šä¸­
            await mcp_manager.remove_connection(server_name)
            await cl.Message(content=f"ğŸ”Œ å·²æ–·ç·š MCP ä¼ºæœå™¨: {server_name}").send()
    
    # æ›´æ–°å·¥å…·åˆ—è¡¨
    await mcp_manager.update_tools()

@cl.step(type="tool", name="MCPå·¥å…·", show_input=True)
async def execute_tool(tool_name: str, tool_input: Dict[str, Any]):
    # å‹•æ…‹è¨­å®šæ­¥é©Ÿåç¨±
    if cl.context.current_step:
        cl.context.current_step.name = f"MCPå·¥å…·: {tool_name}"
    
    mcp_manager = cl.user_session.get('mcp_manager')
    print("Executing tool:", tool_name)
    print("Tool input:", tool_input)
    mcp_name = None
    mcp_tools = mcp_manager.tools

    # æ‰¾åˆ°åŒ…å«æ­¤å·¥å…·çš„ MCP ä¼ºæœå™¨
    for conn_name, tools in mcp_tools.items():
        if any(tool["name"] == tool_name for tool in tools):
            mcp_name = conn_name
            break

    if not mcp_name:
        return {"error": f"Tool '{tool_name}' not found in any connected MCP server"}

    try:
        # ä½¿ç”¨æˆ‘å€‘çš„ MCP é€£ç·šç®¡ç†å™¨å‘¼å«å·¥å…·
        result = await mcp_manager.call_tool(mcp_name, tool_name, tool_input)

        return result
    except Exception as e:
        return {"error": f"Error calling tool '{tool_name}': {str(e)}"}


async def format_tools_for_openai(tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    openai_tools = []

    for tool in tools:
        openai_tool = {
            "type": "function",
            "function": {
                "name": tool["name"],
                "description": tool["description"],
                "parameters": tool["input_schema"],
            },
        }
        openai_tools.append(openai_tool)

    return openai_tools


def format_calltoolresult_content(result):
    """Extract text content from a CallToolResult object.

    The MCP CallToolResult contains a list of content items,
    where we want to extract text from TextContent type items.
    """
    text_contents = []

    if isinstance(result, CallToolResult):
        for content_item in result.content:
            # This script only supports TextContent but you can implement other CallToolResult types
            if isinstance(content_item, TextContent):
                text_contents.append(content_item.text)

    if text_contents:
        return "\n".join(text_contents)
    return str(result)

async def process_llm_response(message_history, initial_msg=None):
    """
    è™•ç† LLM å›ç­”èˆ‡éè¿´å·¥å…·å‘¼å«ï¼Œç›´åˆ°æ²’æœ‰ tool call ç‚ºæ­¢ã€‚
    """
    client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)
    mcp_tools = cl.user_session.get("mcp_manager").tools
    all_tools = []
    for connection_tools in mcp_tools.values():
        all_tools.extend(connection_tools)

    chat_params = {**MODEL_SETTING}
    if all_tools:
        openai_tools = await format_tools_for_openai(all_tools)
        chat_params["tools"] = openai_tools
        chat_params["tool_choice"] = "auto"
        print("Tools passed:", openai_tools)

    # ç”¨æ–¼ streaming å›è¦†
    msg_obj = initial_msg or cl.Message(content="")

    while True:
        stream = await client.chat.completions.create(
            messages=message_history, **chat_params
        )

        response_text = ""
        tool_calls = []

        async for chunk in stream:
            delta = chunk.choices[0].delta
            print(delta)

            if token := delta.content or "":
                response_text += token
                await msg_obj.stream_token(token)

            if delta.tool_calls:
                for tool_call in delta.tool_calls:
                    tc_id = tool_call.index
                    if tc_id >= len(tool_calls):
                        tool_calls.append({"name": "", "arguments": ""})

                    if tool_call.function.name:
                        tool_calls[tc_id]["name"] = tool_call.function.name

                    if tool_call.function.arguments:
                        tool_calls[tc_id]["arguments"] += tool_call.function.arguments

        # å¦‚æœæœ‰ assistant å›è¦†å…§å®¹ï¼ŒåŠ å…¥æ­·å²
        if response_text.strip():
            message_history.append({"role": "assistant", "content": response_text})

        # å¦‚æœæœ‰ tool callï¼ŒåŸ·è¡Œå·¥å…·ä¸¦å°‡çµæœåŠ å…¥æ­·å²ï¼Œç„¶å¾Œ loop å†ä¸Ÿçµ¦ LLM
        if tool_calls:
            for tool_call in tool_calls:
                tool_name = tool_call["name"]
                # è¨˜éŒ„å·¥å…·åŸ·è¡Œå‰çš„æª”æ¡ˆç‹€æ…‹
                file_folder = cl.user_session.get('file_folder')
                existing_files = set()
                if file_folder and os.path.exists(file_folder):
                    existing_files = set(os.listdir(file_folder))                
                try:
                    tool_args = json.loads(tool_call["arguments"])

                    # Add the tool call to message history
                    message_history.append(
                        {
                            "role": "assistant",
                            "content": None,
                            "tool_calls": [
                                {
                                    "id": f"call_{len(message_history)}",
                                    "type": "function",
                                    "function": {
                                        "name": tool_name,
                                        "arguments": tool_call["arguments"],
                                    },
                                }
                            ],
                        }
                    )

                    # Execute the tool in a step
                    tool_result = await execute_tool(tool_name, tool_args)

                    # Format the tool result content
                    tool_result_content = format_calltoolresult_content(tool_result)

                    # Add the tool result to message history
                    message_history.append(
                        {
                            "role": "tool",
                            "tool_call_id": f"call_{len(message_history)-1}",
                            "content": tool_result_content,
                        }
                    )
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„åœ–ç‰‡æª”æ¡ˆç”¢ç”Ÿ
                    await check_and_process_new_images(existing_files)
                except Exception as e:
                    error_msg = f"Error executing tool {tool_name}: {str(e)}"
                    error_message = cl.Message(content=error_msg)
                    await error_message.send()
            # æœ‰ tool callï¼Œç¹¼çºŒ while loopï¼ˆå†ä¸Ÿçµ¦ LLMï¼‰
            # ä¸¦ç”¨æ–°çš„ cl.Message ç‰©ä»¶åš streaming
            msg_obj = cl.Message(content="")
        
            continue
        else:
            # æ²’æœ‰ tool callï¼ŒçµæŸ
            break

    # æ›´æ–° session message history
    cl.user_session.set("message_history", message_history)

@cl.on_message
async def on_message(message: cl.Message):
    new_message =  {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text":  message.content
            }
        ]
    }
    images = [file for file in message.elements if "image" in file.mime]
    
    # å–å¾—è¦–è¦ºèªè¨€æ¨¡å‹è¨­å®š
    current_settings = cl.user_session.get('current_settings', {})
    use_vision_model = current_settings.get("use_vision_model", False)
    
    file_content = [await convert_to_markdown(file.path, use_vision_model=use_vision_model) for file in message.elements if os.path.splitext(file.path)[1] in ['.pdf', '.ppt', '.pptx', '.xls', '.xlsx', '.doc', '.docx']]

    for content in file_content:
        new_message['content'].append(
            {
                "type": "text",
                "text": content
            }
        )
    for image in images:
        new_message['content'].append(
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encode_image(image.path)}",
                    "detail": "high"
                }
            }
        )
    message_history = cl.user_session.get("message_history", [])
    message_history.append(new_message)
    try:
        await process_llm_response(message_history)
    except Exception as e:
        error_message = f"Error: {str(e)}"
        await cl.Message(content=error_message).send()

if __name__ == '__main__':
    from chainlit.cli import run_chainlit
    run_chainlit(__file__)
